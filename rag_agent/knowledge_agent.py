from datetime import datetime
from pathlib import Path
from typing import TypedDict, Literal
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, START, END

from rag_agent.websearch_agent import WebSearchRAG
from utils.agent_utils import read_prompt, print_log
from utils.handle_chromaDB import load_knowledge_base

load_dotenv()

CURRENT_DIR = Path(__file__).resolve().parent
PROMPT_DIR = CURRENT_DIR / "prompt" / "finrag"

SIMILARITY_THRESHOLD = 0.6
WEB_SEARCH_KEYWORDS = ["í˜„ì¬", "ìµœì‹ ", "ì˜¤ëŠ˜", "ì£¼ê°€", "ì‹œì„¸", "ë‰´ìŠ¤", "ì „ë§", "ë‚ ì”¨", "ê²€ìƒ‰í•´ì¤˜", "ì–¼ë§ˆì•¼","ì§€ê¸ˆ","ê²€ìƒ‰","ê²€ìƒ‰í•´"]

llm = ChatOpenAI(model="gpt-5-mini")
web_rag = WebSearchRAG()

# ---------------------------------------------------------
# FinRAG ìƒíƒœ
# ---------------------------------------------------------
class FinRAGState(TypedDict, total=False):
    korean_query: str
    original_query: str
    use_web: bool
    relevant_docs: list
    context_text: str
    citations: list
    final_output: str

# ---------------------------------------------------------
# ë…¸ë“œ
# ---------------------------------------------------------
def node_route(state: FinRAGState) -> dict:
    t0 = print_log("1. ê²€ìƒ‰ ë°©ì‹ ë¼ìš°íŒ… (node_route)", "start")
    korean_query = state["korean_query"]
    use_web = any(kw in korean_query for kw in WEB_SEARCH_KEYWORDS)
    
    extra = f"í‚¤ì›Œë“œ ê°ì§€ë¨ -> ì›¹ ê²€ìƒ‰ ì „í™˜" if use_web else "ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ ì—†ìŒ -> ë‚´ë¶€ DB ê²€ìƒ‰"
    print_log("1. ê²€ìƒ‰ ë°©ì‹ ë¼ìš°íŒ… (node_route)", "end", t0, extra_info=extra)
    return {"use_web": use_web}

def node_web_search(state: FinRAGState) -> dict:
    t0 = print_log("2-A. ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (node_web_search)", "start")
    korean_query = state["korean_query"]
    original_query = state.get("original_query")
    
    web_result = web_rag.web_search(korean_query)
    final_output = web_rag.format_web_result(web_result, original_query, korean_query)
    
    print_log("2-A. ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (node_web_search)", "end", t0, extra_info="ì›¹ ê²€ìƒ‰ ì™„ë£Œ ë° í¬ë§·íŒ…")
    return {"final_output": final_output}

def node_db_retrieve(state: FinRAGState) -> dict:
    t0 = print_log("2-B. ë²¡í„° DB ê²€ìƒ‰ (node_db_retrieve)", "start")
    vs = load_knowledge_base()
        
    korean_query = state["korean_query"]
    relevant_docs = []
    
    if vs:
        try:
            results = vs.similarity_search_with_score(korean_query, k=5)
            print(f"   [Search] '{korean_query}' DB ê²€ìƒ‰ ìˆ˜í–‰")
            for doc, score in results:
                if score <= SIMILARITY_THRESHOLD:
                    relevant_docs.append((doc, score))
                    print(f"      ì±„íƒ: {doc.metadata.get('word')} (ê±°ë¦¬: {score:.4f})")
                else:
                    print(f"      ì œì™¸: {doc.metadata.get('word')} (ê±°ë¦¬: {score:.4f} > {SIMILARITY_THRESHOLD})")
            relevant_docs = relevant_docs[:3]
        except Exception as e:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
            print(f"[{now}] DB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
    print_log("2-B. ë²¡í„° DB ê²€ìƒ‰ (node_db_retrieve)", "end", t0, extra_info=f"ì¡°íšŒëœ ìœ íš¨ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}ê°œ")
    return {"relevant_docs": relevant_docs}

def node_web_fallback(state: FinRAGState) -> dict:
    t0 = print_log("3-A. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± (node_web_fallback)", "start")
    extra = "ë‚´ë¶€ DBì— ê´€ë ¨ ì •ë³´ ì—†ìŒ (ìœ íš¨ ë¬¸ì„œ 0ê°œ) -> ì›¹ ê²€ìƒ‰ ìë™ ì „í™˜"
    print_log("3-A. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± (node_web_fallback)", "end", t0, extra_info=extra)
    return node_web_search(state)

def node_db_answer(state: FinRAGState) -> dict:
    t0 = print_log("3-B. DB ê¸°ë°˜ ë‹µë³€ ìƒì„± (node_db_answer)", "start")
    korean_query = state["korean_query"]
    original_query = state.get("original_query")
    relevant_docs = state.get("relevant_docs") or []
    
    context_text = ""
    citations = []
    for doc, score in relevant_docs:
        word = doc.metadata.get("word", "Term")
        raw_content = doc.page_content
        definition = raw_content.split(":", 1)[1].strip() if ":" in raw_content else raw_content
        context_text += f"- **{word}**: {definition}\n"
        citations.append(f"- **{word}**: {definition[:60]}... (ê±°ë¦¬: {score:.4f})")

    system_template = read_prompt(PROMPT_DIR, "finrag_01_system.md")
    rag_prompt = PromptTemplate.from_template(system_template)
    rag_chain = rag_prompt | llm | StrOutputParser()
    
    try:
        ai_answer = rag_chain.invoke({"context": context_text, "question": korean_query})
    except Exception as e:
        ai_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({e})"

    final_output = f"""
### ğŸŒ ì§ˆë¬¸
- **Original**: {original_query if original_query else korean_query}
- **Translated**: {korean_query}

### ğŸ’¡ FinBotì˜ ë‹µë³€
{ai_answer}

---
### ğŸ“š ë‚´ë¶€ ì°¸ê³  ë¬¸í—Œ
{chr(10).join(citations)}
"""
    print_log("3-B. DB ê¸°ë°˜ ë‹µë³€ ìƒì„± (node_db_answer)", "end", t0)
    return {"final_output": final_output}

def route_after_start(state: FinRAGState) -> Literal["web_search", "db_retrieve"]:
    return "web_search" if state.get("use_web") else "db_retrieve"

def route_after_db(state: FinRAGState) -> Literal["web_fallback", "db_answer"]:
    return "web_fallback" if not (state.get("relevant_docs")) else "db_answer"

# ---------------------------------------------------------
# ê·¸ë˜í”„ ë¹Œë“œ
# ---------------------------------------------------------
_finrag_graph = None

def _get_finrag_graph():
    global _finrag_graph
    if _finrag_graph is None:
        builder = StateGraph(FinRAGState)
        builder.add_node("route", node_route)
        builder.add_node("web_search", node_web_search)
        builder.add_node("db_retrieve", node_db_retrieve)
        builder.add_node("web_fallback", node_web_fallback)
        builder.add_node("db_answer", node_db_answer)

        builder.add_edge(START, "route")
        builder.add_conditional_edges("route", route_after_start, {"web_search": "web_search", "db_retrieve": "db_retrieve"})
        builder.add_edge("web_search", END)
        builder.add_conditional_edges("db_retrieve", route_after_db, {"web_fallback": "web_fallback", "db_answer": "db_answer"})
        builder.add_edge("web_fallback", END)
        builder.add_edge("db_answer", END)
        _finrag_graph = builder.compile()
    return _finrag_graph

def get_rag_answer(korean_query, original_query=None):
    print("\n" + "-"*50)
    total_t0 = print_log("FinRAG ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸", "start")
    
    graph = _get_finrag_graph()
    initial: FinRAGState = {"korean_query": korean_query, "original_query": original_query}
    result = graph.invoke(initial)
    
    print("-"*50)
    print_log("FinRAG ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸", "end", total_t0)
    print("-"*50 + "\n")
    
    return result.get("final_output", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    print(get_rag_answer("ê¸ˆë¦¬ê°€ ë­ì•¼?"))
    print("=" * 60)
    print(get_rag_answer("í˜„ì¬ ì‚¼ì„±ì „ì ì£¼ê°€ ì•Œë ¤ì¤˜"))
    print("=" * 60)
    print(get_rag_answer("ì•„ì´ìœ  ìµœì‹  ì•¨ë²” ë­ì•¼?"))